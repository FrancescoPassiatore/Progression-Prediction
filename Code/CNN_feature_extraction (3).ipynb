{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/extracted.zip\" -d \"/content/drive/MyDrive/osic-pulmonary-fibrosis-progression\"\n"
      ],
      "metadata": {
        "id": "tQCqOwyCbBFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b10d3f-3e43-48f9-a280-33b7d274a987"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN - extract features"
      ],
      "metadata": {
        "id": "0i4swU5yJvkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-process image"
      ],
      "metadata": {
        "id": "nu0QPP4_MGym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IGHniIork1Pp",
        "outputId": "7e156941-bd56-449c-f21d-c30fd0342587"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.7/2.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pydicom\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4MqMg_kLj4UJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
        "import seaborn as sns\n",
        "import math\n",
        "import cv2\n",
        "import pydicom\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import pickle as pkl\n",
        "import matplotlib.image as mpimg\n",
        "from tabulate import tabulate\n",
        "import missingno as msno\n",
        "from IPython.display import display_html\n",
        "from PIL import Image\n",
        "import gc\n",
        "from skimage.transform import resize\n",
        "import copy\n",
        "import re\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Segmentation\n",
        "import glob\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "import scipy.ndimage\n",
        "from skimage import morphology\n",
        "from skimage import measure\n",
        "from skimage.transform import resize\n",
        "from sklearn.cluster import KMeans\n",
        "from plotly import __version__\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "from plotly.tools import FigureFactory as FF\n",
        "from plotly.graph_objs import *\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#------------------------------------------------------------\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "\n",
        "#Define resolution of image 512*512\n",
        "N_ROWS = 512\n",
        "N_COLS = 512\n",
        "path = 'C:/Users/frank/OneDrive/Desktop/Thesis - Progress Prediction/CodeDevelopment/osic-pulmonary-fibrosis-progression/'\n",
        "#Define Batch size\n",
        "BATCH_SIZE=128\n",
        "\n",
        "# Areas with the same number of pixels on the edges are not required. Crop it.\n",
        "def crop_image(img: np.ndarray):\n",
        "    edge_pixel_value = img[0, 0]\n",
        "    mask = img != edge_pixel_value\n",
        "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
        "\n",
        "# Load images, crop thick borders(if any) and resize\n",
        "def load_image(path):\n",
        "    dataset = pydicom.dcmread(path)\n",
        "    img = dataset.pixel_array\n",
        "    img = crop_image(img)\n",
        "#   img = cv2.resize(img, (512,512))\n",
        "    return img\n",
        "\n",
        "# Get Nth percentile image\n",
        "def get_img(perc, patient_id, data):\n",
        "\n",
        "    l = glob.glob(path+'{0}/{1}/*.dcm'.format(data, patient_id))\n",
        "    img_ids = []\n",
        "    for x in l:\n",
        "        y = x.split('\\\\')[-1]\n",
        "        z = int(y.split('.')[0])\n",
        "        img_ids.append(z)\n",
        "\n",
        "    img_ids.sort()\n",
        "\n",
        "    return img_ids[math.ceil(perc*(len(img_ids)))-1]\n",
        "\n",
        "# Get num of slices bw two percentiles\n",
        "def num_img_bw_perc(p1, p2, patient_id, data):\n",
        "\n",
        "    l = glob.glob(path+'{0}/{1}/*.dcm'.format(data, patient_id))\n",
        "    img_ids = []\n",
        "    for x in l:\n",
        "        y = x.split('\\\\')[-1]\n",
        "        z = int(y.split('.')[0])\n",
        "        img_ids.append(z)\n",
        "\n",
        "    img_ids.sort()\n",
        "\n",
        "    return len(img_ids[math.ceil(p1*(len(img_ids)))-1:math.ceil(p2*(len(img_ids)))])-1\n",
        "\n",
        "\n",
        "# Get number of images per patient\n",
        "def get_num_images(patient_id, data):\n",
        "\n",
        "    return len(glob.glob(path+'{0}/{1}/*.dcm'.format(data, patient_id)))\n",
        "\n",
        "# Get the lung area in the image slice\n",
        "def lung_seg_pixel_ratio(img_array):\n",
        "\n",
        "    c = 0\n",
        "    for i in range(img_array.shape[0]):\n",
        "        for j in range(img_array.shape[1]):\n",
        "            if img_array[i][j] != 0:\n",
        "                c+=1\n",
        "\n",
        "    return c, round(c/(img_array.shape[0]*img_array.shape[1]),4)\n",
        "\n",
        "# Get dicom meta data\n",
        "def get_dicom_meta(path):\n",
        "\n",
        "    '''Get information from the .dcm files.\n",
        "    path: complete path to the .dcm file'''\n",
        "\n",
        "    image_data = pydicom.dcmread(path)\n",
        "\n",
        "    # Dictionary to store the information from the image\n",
        "    observation_data = {\n",
        "        \"SliceThickness\" : float(image_data.SliceThickness),\n",
        "        \"PixelSpacing\" : float(image_data.PixelSpacing[0]),\n",
        "    }\n",
        "\n",
        "    return observation_data\n",
        "\n",
        "# Get tissue mask\n",
        "# To extract the tissues from the segmented lung all we need to do is get rid of the border parts from the segmented lung\n",
        "# Grey pixels present within the border of the lung is assumed to be tissue.\n",
        "# Inorder to get rid of the border pixels of the lung we slightly perturb the segmented lung to the right, left, top and bottom\n",
        "# The intersection of all the perturbed images gets rid of the border lung pixels\n",
        "# This resultant image serves as the mask for the tissue segmentation\n",
        "def tissue_mask(img, mask, shift_perc):\n",
        "\n",
        "    r_dim, c_dim = img.shape[0], img.shape[1]\n",
        "\n",
        "    # Move the image by shift_perc to the left\n",
        "    del_left_cols = int(shift_perc*c_dim)\n",
        "\n",
        "    mask1, mask2 = mask.copy(), np.zeros((r_dim, c_dim)).astype(int)\n",
        "    mask1 = mask1[:,del_left_cols:]\n",
        "    mask2[:,:c_dim-del_left_cols] = mask1\n",
        "\n",
        "    # Move the image by shift_perc to the right\n",
        "    del_right_cols = int(shift_perc*c_dim)\n",
        "\n",
        "    mask3, mask4 = mask.copy(), np.zeros((r_dim, c_dim)).astype(int)\n",
        "    mask3 = mask3[:,:c_dim-del_right_cols]\n",
        "    mask4[:,del_right_cols:] = mask3\n",
        "\n",
        "    # Move the image by shift_perc to the top\n",
        "    del_top_rows = int(shift_perc*c_dim) #BUG C_DIM SHOULD BE R_DIM\n",
        "\n",
        "    mask5, mask6 = mask.copy(), np.zeros((r_dim, c_dim)).astype(int)\n",
        "    mask5 = mask5[del_top_rows:,:]\n",
        "    mask6[:r_dim-del_top_rows,:] = mask5\n",
        "\n",
        "    # Move the image by shift_perc to the bottom\n",
        "    del_bottom_rows = int(shift_perc*r_dim)\n",
        "\n",
        "    mask7, mask8 = mask.copy(), np.zeros((r_dim, c_dim)).astype(int)\n",
        "    mask7 = mask7[:r_dim-del_bottom_rows,:]\n",
        "    mask8[del_bottom_rows:,:] = mask7\n",
        "\n",
        "    #Obtain the final mask\n",
        "    final_mask = ((mask2==1) & (mask4==1) & (mask6==1) & (mask8==1)).astype(int)\n",
        "\n",
        "    return final_mask\n",
        "\n",
        "# Get tissue features\n",
        "def tissue_features(tissue_mask, img, thresh = 0.35):\n",
        "\n",
        "    final_img = tissue_mask*img\n",
        "\n",
        "    checker = np.zeros((final_img.shape[0], final_img.shape[1]))\n",
        "    counter, other_counter = 0, 0\n",
        "    for i in range(final_img.shape[0]):\n",
        "        for j in range(final_img.shape[1]):\n",
        "            if final_img[i][j]>=thresh:\n",
        "                checker[i][j] = 1\n",
        "                counter+=1\n",
        "            else:\n",
        "                checker[i][j] = 0\n",
        "                other_counter+=1\n",
        "\n",
        "    tissue_by_total = counter/(final_img.shape[0]**2)\n",
        "    tissue_by_lung = counter/((tissue_mask==1).sum())\n",
        "\n",
        "    return counter, tissue_by_total, tissue_by_lung, checker\n",
        "\n",
        "# https://www.raddq.com/dicom-processing-segmentation-visualization-in-python/\n",
        "def make_lungmask(img, display=False):\n",
        "\n",
        "    img = img.astype(float)\n",
        "    row_size= img.shape[0]\n",
        "    col_size = img.shape[1]\n",
        "\n",
        "\n",
        "    #Normalize image pixels\n",
        "    mean = np.mean(img)\n",
        "    std = np.std(img)\n",
        "    img = img-mean\n",
        "    img = img/std\n",
        "\n",
        "    # Find the average pixel value near the lungs\n",
        "    # to renormalize washed out images\n",
        "    middle = img[int(col_size/5):int(col_size/5*4),int(row_size/5):int(row_size/5*4)]\n",
        "    mean = np.mean(middle)\n",
        "    max = np.max(img)\n",
        "    min = np.min(img)\n",
        "\n",
        "    # To improve threshold finding, I'm moving the\n",
        "    # underflow and overflow on the pixel spectrum\n",
        "    img[img==max]=mean\n",
        "    img[img==min]=mean\n",
        "    #Substitutes too light/dark pixels with mean\n",
        "    # Using Kmeans to separate foreground (soft tissue / bone) and background (lung/air)\n",
        "    kmeans = KMeans(n_clusters=2).fit(np.reshape(middle,[np.prod(middle.shape),1]))\n",
        "    centers = sorted(kmeans.cluster_centers_.flatten())\n",
        "    threshold = np.mean(centers)\n",
        "    thresh_img = np.where(img<threshold,1.0,0.0)  # threshold the image\n",
        "\n",
        "    # First erode away the finer elements, then dilate to include some of the pixels surrounding the lung.\n",
        "    # We don't want to accidentally clip the lung.\n",
        "\n",
        "    eroded = morphology.erosion(thresh_img,np.ones([3,3]))\n",
        "    dilation = morphology.dilation(eroded,np.ones([8,8]))\n",
        "\n",
        "    labels = measure.label(dilation) # Different labels are displayed in different colors\n",
        "    label_vals = np.unique(labels)\n",
        "    regions = measure.regionprops(labels)\n",
        "    good_labels = []\n",
        "    for prop in regions:\n",
        "        B = prop.bbox\n",
        "        if ((B[2]-B[0]<row_size*0.9) and (B[3]-B[1]<col_size*0.9) and (B[2]-B[0]>row_size*0.20)\n",
        "            and (B[3]-B[1]>col_size*0.10) and (B[0]>row_size*0.03) and (B[2]<row_size*0.97)\n",
        "            and (B[1]>col_size*0.03) and (B[3]<col_size*0.97)):\n",
        "            good_labels.append(prop.label)\n",
        "    mask = np.ndarray([row_size,col_size],dtype=np.int8)\n",
        "    mask[:] = 0\n",
        "\n",
        "    #  After just the lungs are left, we do another large dilation\n",
        "    #  in order to fill in and out the lung mask\n",
        "\n",
        "    for N in good_labels:\n",
        "        mask = mask + np.where(labels==N,1,0)\n",
        "    mask = morphology.dilation(mask,np.ones([10,10])) # one last dilation\n",
        " # Compute Lung Area in the slice\n",
        "    lung_pixels, slice_lung_area = lung_seg_pixel_ratio(mask)\n",
        "\n",
        "    # Tissue Mask\n",
        "    t_mask = tissue_mask(img, mask, shift_perc = 0.02)\n",
        "\n",
        "    # Extract tissue features\n",
        "    num_t_pixels, tissue_by_total, tissue_by_lung, checker = tissue_features(t_mask, img, thresh = 0.35)\n",
        "\n",
        "    if (display):\n",
        "        fig, ax = plt.subplots(4, 2, figsize=[18, 18])\n",
        "        ax[0, 0].set_title(\"Original\")\n",
        "        ax[0, 0].imshow(img, cmap='gray')\n",
        "        ax[0, 0].axis('off')\n",
        "        ax[0, 1].set_title(\"Threshold\")\n",
        "        ax[0, 1].imshow(thresh_img, cmap='gray')\n",
        "        ax[0, 1].axis('off')\n",
        "        ax[1, 0].set_title(\"After Erosion and Dilation\")\n",
        "        ax[1, 0].imshow(dilation, cmap='gray')\n",
        "        ax[1, 0].axis('off')\n",
        "        ax[1, 1].set_title(\"Color Labels\")\n",
        "        ax[1, 1].imshow(labels)\n",
        "        ax[1, 1].axis('off')\n",
        "        ax[2, 0].set_title(\"Final Mask\")\n",
        "        ax[2, 0].imshow(mask, cmap='gray')\n",
        "        ax[2, 0].axis('off')\n",
        "        ax[2, 1].set_title(\"Apply Mask on Original\")\n",
        "        ax[2, 1].imshow(mask*img, cmap='gray')\n",
        "        ax[2, 1].axis('off')\n",
        "        ax[3, 0].set_title(\"Inner Lung Mask\")\n",
        "        ax[3, 0].imshow(t_mask, cmap='gray')\n",
        "        ax[3, 0].axis('off')\n",
        "        ax[3, 1].set_title(\"Segmented Tissue\")\n",
        "        ax[3, 1].imshow(checker, cmap='gray')\n",
        "        ax[3, 1].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return img,thresh_img,dilation,labels,mask,mask*img,t_mask,checker\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YNWPAh7tuYkJ",
        "outputId": "9531c5a0-11cb-4f5a-af58-284d8dde9663"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "DST3hWPfVNRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/drive/MyDrive/patient_event_slice.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LJ63Q68KkZ1v",
        "outputId": "73ddcc01-88ed-4453-c86f-0f7eab663807"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Patient  event  time  \\\n",
            "0  ID00007637202177411956430      1    11   \n",
            "1  ID00009637202177434476278      1    52   \n",
            "2  ID00010637202177584971671      1     5   \n",
            "3  ID00011637202177653955184      0    52   \n",
            "4  ID00012637202177665765362      1    54   \n",
            "\n",
            "                                         slice_files  \n",
            "0  ['18.dcm', '19.dcm', '2.dcm', '20.dcm', '21.dc...  \n",
            "1  ['205.dcm', '216.dcm', '226.dcm', '237.dcm', '...  \n",
            "2  ['32.dcm', '35.dcm', '38.dcm', '40.dcm', '43.d...  \n",
            "3  ['18.dcm', '19.dcm', '2.dcm', '20.dcm', '21.dc...  \n",
            "4  ['22.dcm', '24.dcm', '25.dcm', '27.dcm', '28.d...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "df['slice_files'] = df['slice_files'].apply(ast.literal_eval)\n"
      ],
      "metadata": {
        "id": "xEVQfLIZmviR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "patients = df['Patient'].tolist()\n",
        "\n",
        "# Split 70/15/15 stratified by event\n",
        "train_patients, temp_patients = train_test_split(\n",
        "    patients, test_size=0.3, random_state=42, stratify=df['event']\n",
        ")\n",
        "\n",
        "val_patients, test_patients = train_test_split(\n",
        "    temp_patients, test_size=0.5, random_state=42,\n",
        "    stratify=df.set_index('Patient').loc[temp_patients]['event']\n",
        ")\n",
        "\n",
        "train_df = df[df['Patient'].isin(train_patients)].reset_index(drop=True)\n",
        "val_df   = df[df['Patient'].isin(val_patients)].reset_index(drop=True)\n",
        "test_df  = df[df['Patient'].isin(test_patients)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKbcg7OKlDD8",
        "outputId": "2ac32b66-aece-4a3c-9a54-57c2232fb61c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 123, Val: 26, Test: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWgcf5dQleJh",
        "outputId": "6169918b-a4ed-4be5-e6fc-0c536dc52d73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Patient  event  time  \\\n",
            "0    ID00009637202177434476278      1    52   \n",
            "1    ID00010637202177584971671      1     5   \n",
            "2    ID00011637202177653955184      0    52   \n",
            "3    ID00012637202177665765362      1    54   \n",
            "4    ID00015637202177877247924      1     9   \n",
            "..                         ...    ...   ...   \n",
            "118  ID00419637202311204720264      1    12   \n",
            "119  ID00421637202311550012437      0    55   \n",
            "120  ID00422637202311677017371      1    41   \n",
            "121  ID00423637202312137826377      1     1   \n",
            "122  ID00426637202313170790466      0    59   \n",
            "\n",
            "                                           slice_files  \n",
            "0    [205.dcm, 216.dcm, 226.dcm, 237.dcm, 248.dcm, ...  \n",
            "1    [32.dcm, 35.dcm, 38.dcm, 40.dcm, 43.dcm, 46.dc...  \n",
            "2    [18.dcm, 19.dcm, 2.dcm, 20.dcm, 21.dcm, 22.dcm...  \n",
            "3    [22.dcm, 24.dcm, 25.dcm, 27.dcm, 28.dcm, 3.dcm...  \n",
            "4    [259.dcm, 275.dcm, 291.dcm, 307.dcm, 321.dcm, ...  \n",
            "..                                                 ...  \n",
            "118  [17.dcm, 18.dcm, 19.dcm, 2.dcm, 20.dcm, 21.dcm...  \n",
            "119  [26.dcm, 28.dcm, 3.dcm, 31.dcm, 33.dcm, 34.dcm...  \n",
            "120  [227.dcm, 24.dcm, 252.dcm, 265.dcm, 278.dcm, 2...  \n",
            "121  [178.dcm, 185.dcm, 193.dcm, 200.dcm, 208.dcm, ...  \n",
            "122  [209.dcm, 22.dcm, 231.dcm, 242.dcm, 253.dcm, 2...  \n",
            "\n",
            "[123 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "salhogmyl91I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/MyDrive/osic-pulmonary-fibrosis-progression/extracted'\n"
      ],
      "metadata": {
        "id": "U4sT2OvumJRC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Passare alla CNN le immagini elaborate da preprocess per aver immagini piu chiare oltre alle maschere in modo che focalizza sui polmoni\n"
      ],
      "metadata": {
        "id": "9XSdo7keuA3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lifelines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ZASEAs7SzP",
        "outputId": "9cba0d5c-e4ff-4801-be04-f1946c035f19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lifelines\n",
            "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from lifelines) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from lifelines) (1.16.2)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.12/dist-packages (from lifelines) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from lifelines) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.12/dist-packages (from lifelines) (1.8.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines)\n",
            "  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines) (2.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
            "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=3e566c8743580a8d4295c994dd217e4c88977ea4a8a677b2eac854891924f15e\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/37/21/0a719b9d89c635e89ff24bd93b862882ad675279552013b2fb\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-1.2.1 interface-meta-1.3.0 lifelines-0.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import timm\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from lifelines.utils import concordance_index  # pip install lifelines"
      ],
      "metadata": {
        "id": "2JxSSw1PvspR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class\n"
      ],
      "metadata": {
        "id": "gFcaRjVNydkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylibjpeg pylibjpeg-libjpeg python-gdcm\n"
      ],
      "metadata": {
        "id": "A0vgIBa90vMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LungEventDataset(Dataset):\n",
        "\n",
        "  def __init__(self, df, root_path, transform=None, use_masks=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.root_path = root_path\n",
        "        self.transform = transform\n",
        "        self.use_masks = use_masks\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "  def segment_lung(self,img):\n",
        "      img,thresh_img,dilation,labels,mask,mask_img,t_mask,checker = make_lungmask(img)\n",
        "      return mask\n",
        "\n",
        "  def preprocess_slice(self, img):\n",
        "    img,_,_,_,_,_,_,_ = make_lungmask(img)\n",
        "\n",
        "    # Resize to fixed size\n",
        "    img_resized = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # Convert to 3 channels\n",
        "    img_rgb = np.stack([img_resized]*3, axis=0)\n",
        "\n",
        "    # ImageNet normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
        "    std = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
        "    img_rgb = (img_rgb - mean) / std\n",
        "\n",
        "    return img_rgb\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      row = self.df.iloc[index]\n",
        "      patient = row['Patient']\n",
        "      slice_files = row['slice_files']\n",
        "\n",
        "      #Targets\n",
        "      event = torch.tensor(row['event'], dtype= torch.float32)\n",
        "      time = torch.tensor(row['time'], dtype = torch.float32)\n",
        "\n",
        "      #Process slices\n",
        "      slices = []\n",
        "      for f in slice_files:\n",
        "        path = os.path.join(self.root_path,patient,f)\n",
        "\n",
        "\n",
        "        try:\n",
        "          # Read DICOM\n",
        "          ds = pydicom.dcmread(path)\n",
        "          img = ds.pixel_array.astype(np.float32)\n",
        "\n",
        "          img_rgb = self.preprocess_slice(img)\n",
        "          slices.append(img_rgb)\n",
        "        except Exception as e:\n",
        "          print(f'Error:{e}')\n",
        "          continue\n",
        "\n",
        "      if len(slices) == 0:\n",
        "        print(f'\\nNo slices for {patient}')\n",
        "\n",
        "\n",
        "      NUM_SLICES = 11\n",
        "\n",
        "      if len(slices) == 0:\n",
        "          # fallback to zero tensor\n",
        "          slices = np.zeros((NUM_SLICES, 3, 224, 224), dtype=np.float32)\n",
        "      else:\n",
        "          slices = np.stack(slices)\n",
        "          if slices.shape[0] < NUM_SLICES:\n",
        "              pad_shape = (NUM_SLICES - slices.shape[0], *slices.shape[1:])\n",
        "              slices = np.concatenate([slices, np.zeros(pad_shape, dtype=slices.dtype)], axis=0)\n",
        "          elif slices.shape[0] > NUM_SLICES:\n",
        "              slices = slices[:NUM_SLICES]\n",
        "\n",
        "      slices = torch.from_numpy(slices).float()\n",
        "\n",
        "      return{\n",
        "          'slices':slices,\n",
        "          'event':event,\n",
        "          'time':time,\n",
        "          'patient':patient,\n",
        "      }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u9YtwNJLvlC6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "Vnb91o9zygci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurvivalModel(nn.Module):\n",
        "  #Add here clinical_Dim to insert handcrafted features\n",
        "  def __init__(self, backbone_name='efficientnet_b0',pretrained = True, use_attention = True):\n",
        "    super().__init__()\n",
        "\n",
        "    #Imaging branch\n",
        "    self.backbone = timm.create_model(backbone_name,pretrained=pretrained,in_chans=3,num_classes=0)\n",
        "\n",
        "    self.feature_dim = self.backbone.num_features\n",
        "    #Attention to weight the importance of slices\n",
        "    self.use_attention = use_attention\n",
        "    if use_attention:\n",
        "      self.slice_attention = nn.Sequential(nn.Linear(self.feature_dim,128), nn.Tanh(),nn.Linear(128,1))\n",
        "\n",
        "    #Clinical branch for later\n",
        "\n",
        "    #Output head\n",
        "    self.risk_head = nn.Linear(self.feature_dim,1) #Cox risk\n",
        "\n",
        "  def forward(self, slices):\n",
        "    B,N,C,H,W = slices.shape\n",
        "    slices_flat = slices.view(B*N,C,H,W)\n",
        "\n",
        "    # Extract features\n",
        "    img_features = self.backbone(slices_flat)  # [B*N, feature_dim]\n",
        "    img_features = img_features.view(B,N,self.feature_dim)  # [B,N,feature_dim]\n",
        "\n",
        "    if self.use_attention:\n",
        "        attention_logits = self.slice_attention(img_features)\n",
        "        attention_weights = F.softmax(attention_logits,dim=1)\n",
        "        img_pooled = (img_features * attention_weights).sum(dim=1)\n",
        "    else:\n",
        "        img_pooled = img_features.mean(dim=1)  # [B, feature_dim]\n",
        "\n",
        "    # Optional reduction\n",
        "    # img_pooled = F.relu(self.fc_reduce(img_pooled))  # if using intermediate layer\n",
        "\n",
        "    risk_score = self.risk_head(img_pooled)  # [B,1]\n",
        "\n",
        "    return risk_score\n",
        "\n"
      ],
      "metadata": {
        "id": "9Qiscmv3yhsF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COX LIKELIHOOD LOSS"
      ],
      "metadata": {
        "id": "EjjiMt_p1SyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cox_loss(risk_scores,times,events):\n",
        "  sorted_indices = torch.argsort(times,descending=True)\n",
        "  risk_scores = risk_scores[sorted_indices]\n",
        "  events = events[sorted_indices]\n",
        "\n",
        "  hazard_ratio = torch.exp(risk_scores)\n",
        "  log_risk = torch.log(torch.cumsum(hazard_ratio,dim=0)+ 1e-8)\n",
        "\n",
        "  uncensored_likelihood = risk_scores - log_risk\n",
        "  loss = -torch.sum(uncensored_likelihood*events)/ (torch.sum(events)+ 1e-8)\n",
        "\n",
        "  return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "95jhI5-5zkDW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN"
      ],
      "metadata": {
        "id": "QPO1PjFn2qI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = torch.cuda.amp.GradScaler()  # initialize if not passed\n",
        "\n",
        "    for batch in tqdm(loader, desc='Training'):\n",
        "        slices = batch['slices'].to(device)\n",
        "        events = batch['event'].to(device)\n",
        "        times= batch['time'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            risk_scores = model(slices).squeeze(1)\n",
        "            loss = cox_loss(risk_scores,times, events)\n",
        "\n",
        "        # Backward with scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader), scaler\n"
      ],
      "metadata": {
        "id": "XRMy3lac2p0d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_risks, all_times,all_events = [], [], []\n",
        "\n",
        "    for batch in tqdm(loader, desc='Validation'):\n",
        "        slices = batch['slices'].to(device)\n",
        "        events = batch['event'].to(device)\n",
        "        times = batch['time'].to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            risk_scores = model(slices).squeeze(1)\n",
        "\n",
        "        all_risks.append(risk_scores.cpu())\n",
        "        all_events.append(events.cpu())\n",
        "        all_times.append(times.cpu())\n",
        "\n",
        "    all_risks = torch.cat(all_risks)\n",
        "    all_events = torch.cat(all_events)\n",
        "\n",
        "    c_index = concordance_index(all_times.numpy(), -all_risks.numpy(),all_events.numpy())\n",
        "    return c_index, all_risks,all_times, all_events"
      ],
      "metadata": {
        "id": "qWwFXYsb2vLT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "sxK82jrT5btj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#Load data -> already have train_df,val_df,test_df\n",
        "train_dataset = LungEventDataset(train_df,root_path=root_path,use_masks=True)\n",
        "val_dataset = LungEventDataset(val_df, root_path=root_path,use_masks=True)\n",
        "\n",
        "#DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8,shuffle=True,num_workers=4)\n",
        "val_loader = DataLoader(val_dataset,batch_size=8,shuffle=False,num_workers=4)\n",
        "\n",
        "#Model\n",
        "\n",
        "model = SurvivalModel( backbone_name='efficientnet_b0',pretrained=True,use_attention=True).to(device)\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Model initialized\")\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "best_c_index = 0.0\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, scaler = train_epoch(model, train_loader, optimizer, device, scaler)\n",
        "\n",
        "    # Validate\n",
        "    val_c_index, val_risks, val_times, val_events = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val C-index: {val_c_index:.4f}\")\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_c_index)\n",
        "\n",
        "    # Save best model\n",
        "    if val_c_index > best_c_index:\n",
        "        best_c_index = val_c_index\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'c_index': val_c_index,\n",
        "        }, 'best_survival_model.pth')\n",
        "        print(f\"‚úÖ Saved best model (C-index: {val_c_index:.4f})\")\n",
        "\n",
        "print(f\"\\nüéâ Training completed!\")\n",
        "print(f\"   Best C-index: {best_c_index:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jkhHk-Uw5bUh",
        "outputId": "e5af9475-16c3-4c2e-faa9-609cbebfc484"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "‚úÖ Model initialized\n",
            "   Parameters: 4,172,926\n",
            "\n",
            "============================================================\n",
            "Epoch 1/30\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [10:21<02:03, 30.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "Error:Unable to decompress 'JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])' pixel data because all plugins are missing dependencies:\n",
            "\tgdcm - requires gdcm>=3.0.10\n",
            "\tpylibjpeg - requires pylibjpeg>=2.0 and pylibjpeg-libjpeg>=2.1\n",
            "\n",
            "No slices for ID00011637202177653955184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [11:46<01:40, 50.44s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-1183870778.py\", line 64, in __getitem__\n    slices = np.stack(slices)  # original\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/shape_base.py\", line 444, in stack\n    raise ValueError('need at least one array to stack')\nValueError: need at least one array to stack\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3768062121.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1575294955.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device, scaler)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initialize if not passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-1183870778.py\", line 64, in __getitem__\n    slices = np.stack(slices)  # original\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/shape_base.py\", line 444, in stack\n    raise ValueError('need at least one array to stack')\nValueError: need at least one array to stack\n"
          ]
        }
      ]
    }
  ]
}